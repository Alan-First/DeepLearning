{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- encoding: utf-8 -*-\n",
    "'''\n",
    "@File         :AutoModel.ipynb\n",
    "@Description  :用automodel方式完成多种NLP任务\n",
    "@Time         :2022/04/28 20:40:19\n",
    "@Author       :Hedwig\n",
    "@Version      :1.0\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoModel类用于管理Transformers库中处理相同地NLP任务的底层具体模型，为上层应用管道方式提供统一的接口\n",
    "# 按Bertology系列的应用场景，Transformer库被划分为以下6个子类\n",
    "# AutoModel：基本载入类，适用于Transformers中的任何模型，也适用于特征提取\n",
    "# AutoModelForPreTraining：特征提取任务的模型载入类，适用于Transformers库中所有的预训练模型\n",
    "# AutoModelForSequenceClassification：文本分类的模型载入类，适用于Transformers中所有文本分类模型\n",
    "# AutoModelForQuestionAnswering：阅读理解任务的模型载入类，适用于Transformers中所有抽取式问答模型\n",
    "# AutoModelWithLMHead：完形填空任务的模型载入类，适用于Transformers中所有遮蔽语言模型\n",
    "# AutoModelForTokenClassification：实体词识别的模型载入类，适用于Transformers库中所有实体词识别模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformers的models/auto路径下有modeling_auto.py源文件，可以找到模型载入类和具体的模型映射关系\n",
    "# 以AutoModelWithLMHead类为例，MODEL_WITH_LM_HEAD_MAPPING_NAMES代表了载入类与系列模型的映射\n",
    "# 在这里列出的所有元素都可以实现AutoModelWithLMHead类所完成的完形填空\n",
    "# 它的元素包括两部分，具体模型的配置文件和具体模型的实现类\n",
    "# 每一个具体模型的实现类会通过不同数据集被训练成多套预训练模型文件\n",
    "# 每套模型训练文件又由3-4个子文件组成：词表文件、词表扩展文件(可选)、配置文件和模型权值文件\n",
    "# 这些文件共用一个统一的字符串标识\n",
    "# 用自动加载方式调用模型时，系统会根据统一的预训练模型字符串标识，找到对应的预训练模型文件，通过网络下载并载入 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 具体地，pipeline的任务字符串(以fill-mask为例)传入后，pipeline会把传入的任务作为SUPPORTED_TASKS的键，\n",
    "# 找到对应的配置字典，用get_default_model方法根据配置字典中default键找到相应model的模型字符串\n",
    "# (这里是distilroberta-base)，这个字符串用于infer_framework_load_model函数转化为模型的类\n",
    "# 在默认的SUPPORTED_TASKS中就是AutoModelForMaskedLM类 \n",
    "# 同时还会用配置字典的imple键找到子pipeline的类名(这里是FillMaskPipeline，如果pipeline_class有指定就用\n",
    "# 指定的这个)，并调用它的call方法，将模型的类作为参数传入\n",
    "# 模型的类又调用相应的任务，(在这里是RobertaForMaskedLM)，加载相应的配置文件\n",
    "# transformmer文件下的modelcards.py有任务字符串的键(fill-mask)与对应的\n",
    "# (MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING_NAMES)的映射\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /home/mist/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /home/mist/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /home/mist/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /home/mist/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
      "loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /home/mist/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'who', 'is', 'Li', 'Jin', '##hong', '?', '[SEP]', 'Li', 'Jin', '##H', '##ong', 'is', 'a', 'programmer', '[SEP]']\n",
      "['[CLS]', 'who', 'is', 'Li', 'Jin', '##hong', '?', '[SEP]', '[MASK]', 'Jin', '##H', '##ong', 'is', 'a', 'programmer', '[SEP]']\n",
      "tensor([[  101,  1150,  1110,  5255, 10922, 15564,   136,   102,   103, 10922,\n",
      "          3048,  4553,  1110,   170, 23981,   102]])\n",
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /home/mist/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /home/mist/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted token is: Li\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import *\n",
    "model_identity = 'bert-base-cased'\n",
    "# 'distilbert-base-uncased','bert-base-cased'亲测好用\n",
    "# 加载词表文件\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_identity)#'bert-base-uncased'\n",
    "# tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "# 输入文本\n",
    "text = \"[CLS] who is Li Jinhong ? [SEP] Li JinHong is a programmer [SEP]\"\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "print(tokenized_text)\n",
    "# 屏蔽部分单词并将其转换成索引值\n",
    "masked_index = 8#掩码位置\n",
    "tokenized_text[masked_index] = '[MASK]'\n",
    "print(tokenized_text)\n",
    "# 将标识转换为词汇表索引\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "#print(indexed_tokens)\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "print(tokens_tensor)\n",
    "# 指定设备\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "# 加载预训练模型\n",
    "# model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "# 也可以用下一行AutoModelWithLMHead替换上一行\n",
    "model = AutoModelWithLMHead.from_pretrained(model_identity)\n",
    "\n",
    "# 如果用手动方式，记得config=AutoConfig.from_pretrained('手动路径')\n",
    "# 如果想换用别的模型标识符，打开transformers下的model文件夹选择想要的文件夹\n",
    "# tokenization文件里有模型标识符，修改token即可\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "# 段标识索引\n",
    "segments_ids = [0]*8+[1]*7\n",
    "segments_tensor = torch.tensor([segments_ids]).to(device)\n",
    "tokens_tensor = tokens_tensor.to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor)#,token_type_ids=segments_tensor\n",
    "predictions = outputs[0]# [1,15,30522]\n",
    "predict_index = torch.argmax(predictions[0,masked_index]).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predict_index])[0]\n",
    "print('Predicted token is:',predicted_token)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
