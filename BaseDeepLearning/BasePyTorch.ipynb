{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.9.0a0+gitd69c22d'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 用于检验pytorch安装是否成功的常规操作\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8360, 0.9117, 0.7324],\n",
       "        [0.2278, 0.6764, 0.2234],\n",
       "        [0.6801, 0.9425, 0.5561],\n",
       "        [0.0258, 0.2963, 0.0823],\n",
       "        [0.7544, 0.7261, 0.4810]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建Tensor的常用方式，常用于生成随机tensor作为输入测试搭好的神经网络模型内部是否维度匹配\n",
    "torch.rand(5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 常用创建tensor的方式\n",
    "x=torch.zeros(5,3,dtype=torch.long)# 创建全零张量，设置类型，还有其他可选torch.float64\n",
    "x=torch.Tensor([5,3])# 把列表[5,3]转化为张量\n",
    "y=x.new_ones(2,4)# 以x为模板创建一个2行4列的全1张量，这个张量与x有相同的torch.dtype和torch.device\n",
    "z=torch.randn_like(x,dtype=torch.float64)# 创建与x相同尺寸随机数\n",
    "x.size()# 获取tensor尺寸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.1860140562057495\n"
     ]
    }
   ],
   "source": [
    "x=torch.randn(1)\n",
    "print(x.item())# Tensor元素只有一个的时候用于获取Tensor的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.randn(2,2)\n",
    "y=torch.randn(2,2)\n",
    "z = torch.add(x,y)# 加法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor可以理解为一个变量，也就是说当创建一个Tensor(1)的时候，最后理解成创建了一个自变量x，x的取值为1，这样方便理解对该张量的求导，比如最后它与张量Tensor(2)相乘，就可以理解成另一个自变量y它相乘，得到函数z=x*y，这时候对Tensor(1)求导即为z对x求导，导数是y，而y=2，所以该导数为2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.4969,  0.3869,  0.1799],\n",
      "        [-0.2547,  0.6612,  0.5817],\n",
      "        [-0.1369,  1.0073, -0.4423],\n",
      "        [ 0.3169,  1.4155,  0.8150],\n",
      "        [ 0.0336,  0.6081, -1.0811]])\n",
      "tensor([-1.4969,  0.3869,  0.1799, -0.2547,  0.6612,  0.5817, -0.1369,  1.0073,\n",
      "        -0.4423,  0.3169,  1.4155,  0.8150,  0.0336,  0.6081, -1.0811])\n",
      "tensor([[-1.4969,  0.3869,  0.1799, -0.2547,  0.6612],\n",
      "        [ 0.5817, -0.1369,  1.0073, -0.4423,  0.3169],\n",
      "        [ 1.4155,  0.8150,  0.0336,  0.6081, -1.0811]])\n"
     ]
    }
   ],
   "source": [
    "x=torch.randn(5,3)\n",
    "z=x.view(15)# view函数得到的变量和x是共享内存的，也就是它们其实指向同一个Tensor，只不过观察角度不同\n",
    "z1=x.view(-1,5)\n",
    "print(x)\n",
    "print(z)# z是把x按行观察变成一个行向量\n",
    "print(z1)# view第一个参数-1说明按张量尺寸确定，第二个参数5说明列必须是5列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3]])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4]])\n",
      "tensor([[2, 3, 4],\n",
      "        [3, 4, 5],\n",
      "        [4, 5, 6],\n",
      "        [5, 6, 7]])\n"
     ]
    }
   ],
   "source": [
    "# 不同尺寸的Tensor相加时会自动触发广播机制，也就是尺寸向大尺寸对齐\n",
    "# 1）数组看齐维度最长的数组，维度不足部分在前加1补齐\n",
    "# 2）运算后的数组维度是所有数组维度的最大值\n",
    "# 3）输入数组的某个维度和输出数组的对应维度长度相同或长度为1，则数组可以计算，否则会出错\n",
    "# 4）输入数组某维度长度为1时，其他维度都用这一维度的值\n",
    "x=torch.arange(1,4).view(1,3)# 生成[1,4)的数\n",
    "y=torch.arange(1,5).view(4,1)\n",
    "print(x)\n",
    "print(y)\n",
    "print(x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "y= [[1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "x= tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "y= [[2. 2. 2.]\n",
      " [2. 2. 2.]\n",
      " [2. 2. 2.]\n",
      " [2. 2. 2.]\n",
      " [2. 2. 2.]]\n",
      "x= tensor([[3., 3., 3.],\n",
      "        [3., 3., 3.],\n",
      "        [3., 3., 3.],\n",
      "        [3., 3., 3.],\n",
      "        [3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "x=torch.ones(5,3)\n",
    "y=x.numpy()# 采用numpy方法将x转为numpy，二者内存共享\n",
    "print('x=',x)\n",
    "print('y=',y)\n",
    "x+=1\n",
    "print('x=',x)\n",
    "print('y=',y)# 可以发现我们只对x加1，但是y也随之改变，这是因为二者是共享内存的\n",
    "z=torch.from_numpy(y)# 该方法可以将numpy转Tensor，同样的，转出来的结果也共享内存\n",
    "z+=1\n",
    "print('x=',x)# 可以发现只对z加1，x的值却变了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.6977,  0.1552,  0.3303,  1.2030,  1.5942],\n",
      "        [ 0.6315,  1.3783,  2.1998,  0.5334,  1.2236],\n",
      "        [ 1.0338,  1.3130,  0.3557, -0.7227,  1.2405]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# pytorch在CPU和GPU的应用\n",
    "x=torch.randn(3,5)\n",
    "if torch.cuda.is_available():# 用于判断cuda是否可见和可用的常用判断\n",
    "    device=torch.device('cuda')# cuda设备设置\n",
    "    y=torch.ones_like(x,device=device)# 直接在device的GPU上创建Tensor\n",
    "    x=x.to(device)# 把x移动到cuda\n",
    "    z=x+y\n",
    "    print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "<AddBackward0 object at 0x7f1406fc1240>\n"
     ]
    }
   ],
   "source": [
    "# 自动求解梯度\n",
    "x=torch.randn(5,3,requires_grad=True)# 创建一个需要追踪梯度的张量\n",
    "# 这个用于查看创建该Tensor的Function（CalculateGraph.py有介绍这个函数），说明该变量是从哪个运算得到的\n",
    "# 由于我们直接创建，所以它是None\n",
    "print(x.grad_fn)\n",
    "y=x+1\n",
    "print(y.grad_fn)# y是自变量x加上常数得到的，通过加法创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad of x= False\n",
      "grad of z= True\n"
     ]
    }
   ],
   "source": [
    "# 修改张量求解梯度与否\n",
    "x=torch.randn(5,3)\n",
    "y=(x*3)/(x-1)# 默认创建时没有跟踪梯度\n",
    "print('grad of x=',x.requires_grad)\n",
    "x.requires_grad_(True)# 跟踪梯度\n",
    "z=(x*x).sum()# y可用求导\n",
    "print('grad of z=',z.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= tensor([[-0.4376,  1.3637,  0.3737],\n",
      "        [-1.2171,  0.4182,  0.8781],\n",
      "        [-0.1474,  0.8405, -1.4764],\n",
      "        [-0.1416, -1.6874,  0.6844],\n",
      "        [-0.0931, -1.3720, -0.3416]], requires_grad=True)\n",
      "x grad= tensor([[-0.8751,  2.7275,  0.7475],\n",
      "        [-2.4343,  0.8363,  1.7562],\n",
      "        [-0.2948,  1.6810, -2.9527],\n",
      "        [-0.2833, -3.3747,  1.3688],\n",
      "        [-0.1863, -2.7440, -0.6833]])\n"
     ]
    }
   ],
   "source": [
    "# 对上面的z求梯度\n",
    "z.backward()\n",
    "# x*x的导数是2*x，此时x已经有了梯度\n",
    "print('x=',x)\n",
    "print('x grad=',x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x**2 require grad= True\n",
      "x**2 require grad= False\n",
      "x require grad= True\n"
     ]
    }
   ],
   "source": [
    "x=torch.rand(5,3,requires_grad=True)\n",
    "print('x**2 require grad=',(x**2).requires_grad)\n",
    "with torch.no_grad():\n",
    "    print('x**2 require grad=',(x**2).requires_grad)# 用代码段以停止跟踪x接下来的运算的梯度\n",
    "    print('x require grad=',x.requires_grad)# 注意只是不跟踪x的运算的梯度，x的梯度还在\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文献7中的pytorch编程基础"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [0, 8, 0],\n",
       "       [0, 0, 4]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用基本的numpy\n",
    "# 创建对角矩阵\n",
    "v = np.array([1,8,4])\n",
    "np.diag(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.eye(3)# 创建单元矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(range(4)).reshape(2,2)\n",
    "b = np.array(range(4,8)).reshape(2,2)\n",
    "print(a*b)# 哈达玛积\n",
    "print(a@b)# 点积\n",
    "print(np.dot(a,b))# 点积\n",
    "ma = np.asmatrix(a)\n",
    "mb = np.asmatrix(b)\n",
    "print(ma*mb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对角矩阵特性\n",
    "a = np.diag([1,2,3])\n",
    "print(a)\n",
    "v,e = np.linalg.eig(a)# 计算特征值和特征向量\n",
    "print(v)# 特征值，对角矩阵特征值就是其对角\n",
    "print(e)\n",
    "print(a@a@a)# 对角矩阵哈达玛积跟点积一样的结果\n",
    "print(np.linalg.inv(a))# 取逆\n",
    "print(np.matrix(a).I)# 转矩阵以后可直接取逆"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建张量\n",
    "a = torch.tensor(5)# 创建单个张量\n",
    "print(a)\n",
    "anp = np.asarray([4])\n",
    "a = torch.tensor(anp)\n",
    "print(a)\n",
    "# 注意torch.Tensor和torch.tensor使用的区别\n",
    "a = torch.Tensor(5)# 这个指定的是尺寸而非内容，张量有值但是其实算没有初始化的\n",
    "print(a)\n",
    "a = torch.Tensor(1,2)# 这个也指定的是尺寸而非内容\n",
    "print(a)\n",
    "a = torch.Tensor([5])# 这个指定的是内容\n",
    "print(a)\n",
    "# 如果想指定随机初始化值\n",
    "x = torch.rand(2,1)\n",
    "print(x)\n",
    "zero = torch.zeros_like(x)# 生成与x形状且类型一样的0张量，类似的还有ones_like\n",
    "print(zero)\n",
    "eye = torch.eye(2)# 对角矩阵\n",
    "print(eye)\n",
    "full = torch.full((2,2),3)# 前一个参数是尺寸，后1个是值\n",
    "print(full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置默认的张量类型\n",
    "print(torch.get_default_dtype())\n",
    "print(torch.Tensor([1,3]).dtype)\n",
    "torch.set_default_dtype(torch.float64)\n",
    "print(torch.get_default_dtype())\n",
    "print(torch.Tensor([1,3]).dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16454425952403744961\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2022"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 生成随机张量\n",
    "print(torch.initial_seed())\n",
    "torch.manual_seed(2022)\n",
    "torch.initial_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1110,  0.2050,  0.0333],\n",
      "        [-1.0556, -0.5344,  1.2262]])\n",
      "tensor([1, 3, 5, 7, 9])\n",
      "tensor([1., 3., 5., 7., 9.])\n",
      "tensor([1.0000e+01, 1.0000e+03, 1.0000e+05, 1.0000e+07, 1.0000e+09])\n",
      "tensor([[5.2290e-316, 5.2276e-316]])\n"
     ]
    }
   ],
   "source": [
    "# 随机生成张量\n",
    "print(torch.randn(2,3))# 指定生成尺寸\n",
    "print(torch.arange(1,10,step=2))# 线性空间随机值，半闭半开区间，按步长取数\n",
    "print(torch.linspace(1,9,steps=5))# 线性空间随机值，闭区间，取step个数\n",
    "print(torch.logspace(1,9,steps=5))# 对数空间随机值\n",
    "print(torch.empty(1,2))# 生成未初始化矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6.9402e-310, 5.1747e-316])\n",
      "2\n",
      "True\n",
      "tensor([4], dtype=torch.int32)\n",
      "tensor([4.])\n",
      "tensor([4], dtype=torch.int32)\n",
      "tensor([4.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor(2)\n",
    "print(a)\n",
    "print(torch.numel(a))# 取出a中元素的个数2\n",
    "print(torch.is_tensor(a))# 判断是不是张量\n",
    "a = torch.FloatTensor([4])\n",
    "print(a.type(torch.IntTensor)) # 修改tensor类型\n",
    "print(a.type(torch.DoubleTensor))\n",
    "print(a.int())# 数值张量可以用类似这种写法\n",
    "print(a.double())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.], dtype=torch.float32)\n",
      "tensor([8.], dtype=torch.float32)\n",
      "tensor([12.], dtype=torch.float32)\n",
      "tensor(12., dtype=torch.float32)\n",
      "tensor([3.4641], dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# 运算重载\n",
    "a = torch.FloatTensor([4])\n",
    "print(a)\n",
    "b = torch.add(a,a)\n",
    "print(b)\n",
    "# 自变化函数\n",
    "a.add_(b)# a+=b\n",
    "print(a)\n",
    "# pytorch所有的自变化函数都会带有下划线，如x.copy_(y)\n",
    "print(a.mean())\n",
    "print(a.sqrt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 张量与numpy互换\n",
    "a = torch.FloatTensor([4])\n",
    "print(a.numpy())# 张量转numpy\n",
    "anp = np.asarray([4])\n",
    "print(torch.from_numpy(anp))# numpy转tensor\n",
    "print(torch.tensor(anp))# numpy转tensor\n",
    "# 张量与numpy各自形状获取\n",
    "x = torch.rand(2,1)\n",
    "print(x.shape)\n",
    "print(x.size())\n",
    "anp = np.asarray([4,2])\n",
    "print(anp.size,anp.shape)\n",
    "print(x.reshape([1,2]).shape)# 修改形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1])\n",
      "tensor([2, 2])\n",
      "tensor([2, 2])\n"
     ]
    }
   ],
   "source": [
    "# 注意：numpy在转换成张量以后两个变量共享同一块内存，其中一个变量的改变会影响另一个值\n",
    "# pytorch内部实现时，numpy转张量后，如果张量改动，会出发复制机制，不会改变numpy值\n",
    "# 但是numpy转张量以后对numpy修改，由于numpy没有这种机制，所以会导致tensor也变化\n",
    "nparray = np.array([1,1])\n",
    "x = torch.from_numpy(nparray)# x来自于nparray，内存共享\n",
    "print(x)\n",
    "nparray+=1# 修改的是nparray\n",
    "print(x)# x却改变了\n",
    "nparray = nparray+1# 这次也改变了nparray\n",
    "print(x)#但是x没有改变，这是因为上一行执行时候是复制了一个内存存储nparray+1的值\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.], device='cuda:0', dtype=torch.float32)\n",
      "tensor([4.], dtype=torch.float32)\n",
      "tensor([4], device='cuda:0')\n",
      "tensor([4.], device='cuda:0', dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# 在cpu和gpu中定义张量\n",
    "a = torch.FloatTensor([4])\n",
    "b = a.cuda()\n",
    "print(b)\n",
    "print(b.cpu())# 移动数据\n",
    "a = torch.tensor([4],device='cuda:0')# 直接调用,此时只能用tensor方法创建\n",
    "print(a)\n",
    "a = torch.FloatTensor([4])\n",
    "print(a.to(\"cuda:0\"))\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"0\"# 定义gpu环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4])\n",
      "torch.Size([1, 1, 4])\n",
      "torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "# 张量维度变化\n",
    "a = torch.tensor([[1,2],[3,4]])\n",
    "b = torch.reshape(a,(1,-1))\n",
    "print(b.shape)\n",
    "c = torch.unsqueeze(b,dim=0)# 增加值为1的维度\n",
    "print(c.shape)\n",
    "print(torch.squeeze(c,dim=0).shape)# 消除值为1的维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 矩阵转置\n",
    "b = torch.tensor([[5,6,7],[2,8,0]])\n",
    "print(torch.t(b))\n",
    "print(b.permute(1,0))# 维度0和1交换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "True\n",
      "tensor([5, 2, 6, 8, 7, 0])\n"
     ]
    }
   ],
   "source": [
    "# 张量的维度变化\n",
    "# view方法只能操作连续内存的张量，permute或transpose以后的张量内存可能被修改\n",
    "# 可以用is_contiguous方法判断内存是否连续，用contiguous方法使内存连续\n",
    "b = torch.tensor([[5,6,7],[2,8,0]])\n",
    "print(b.is_contiguous())\n",
    "c = b.transpose(0,1)\n",
    "print(c.is_contiguous())\n",
    "print(c.contiguous().is_contiguous())\n",
    "print(c.contiguous().view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 张量连接\n",
    "a = torch.tensor([[1,2],[3,4]])\n",
    "b = torch.tensor([[5,6],[7,8]])\n",
    "print(torch.cat([a,b],dim=0))\n",
    "print(torch.cat([a,b],dim=1))\n",
    "# 张量切割\n",
    "print(torch.chunk(a,chunks=2,dim=0))# 沿着0维度切成两份\n",
    "print(torch.chunk(a,chunks=2,dim=1))\n",
    "# 张量不均匀切割\n",
    "b = torch.tensor([[5,6,7],[2,8,0]])\n",
    "print(torch.split(b,split_size_or_sections=(1,2),dim=1))# 将b沿着1维度切成两部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6, 5],\n",
      "        [8, 0]])\n",
      "tensor([[2, 6, 7]])\n",
      "tensor([[2, 8, 0]])\n"
     ]
    }
   ],
   "source": [
    "# 数据检索\n",
    "b = torch.tensor([[5,6,7],[2,8,0]])\n",
    "print(torch.gather(b,dim=1,index=torch.tensor([[1,0],[1,2]])))# 维持维度1按index输出\n",
    "print(torch.gather(b,dim=0,index=torch.tensor([[1,0,0]])))# 维持维度0按index输出\n",
    "# 如果要取整行可以用index_select\n",
    "print(torch.index_select(b,dim=0,index=torch.tensor(1)))# 维持维度0大小不变，取每个其他维度idx=1的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False,  True],\n",
      "        [ True,  True]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([2, 3, 4])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[1,2],[3,4]])\n",
    "mask = a.ge(2)# 大于等于ge 大于gt 小于等于le 小于lt\n",
    "print(mask)\n",
    "torch.masked_select(a,mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1, 6, 7],\n",
       "        [1, 8, 1]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 条件张量取值\n",
    "b = torch.tensor([[5,6,7],[2,8,0]])\n",
    "c = torch.ones_like(b)\n",
    "print(c)\n",
    "torch.where(b>5,b,c)# 条件满足选b不满足选c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 2],\n",
       "        [3, 3]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 阈值截断\n",
    "a = torch.tensor([[1,2],[3,4]])\n",
    "torch.clamp(a,min=2,max=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 0, 0])\n",
      "tensor([1, 0, 1, 1])\n",
      "torch.return_types.max(\n",
      "values=tensor([7, 4, 5, 8]),\n",
      "indices=tensor([0, 1, 0, 0]))\n"
     ]
    }
   ],
   "source": [
    "# 获取最值索引\n",
    "a = torch.tensor([[7,2,5,8],[3,4,1,6]])\n",
    "print(torch.argmax(a,dim=0))# 维持0维大小\n",
    "print(torch.argmin(a,dim=0))\n",
    "print(torch.max(a,dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# 求导机制\n",
    "x = torch.tensor([1.],requires_grad=True)# 跟踪梯度\n",
    "with torch.no_grad():\n",
    "    y=x*2\n",
    "print(y.requires_grad)\n",
    "#@torch.no_grad()装饰器可以对函数实现这种功能\n",
    "# 激活梯度则为torch.enable_grad\n",
    "# 统一管理torch.set_grad_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6189, 0.9817],\n",
      "        [0.3301, 0.9969]], requires_grad=True) None\n",
      "<AddBackward0 object at 0x7f5f0656dba8>\n",
      "(tensor([[0.6189, 0.9817],\n",
      "        [0.3301, 0.9969]], requires_grad=True), None)\n",
      "False\n",
      "True\n",
      "tensor(2.7319, grad_fn=<MeanBackward0>) tensor([[0.2500, 0.2500],\n",
      "        [0.2500, 0.2500]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.6189387, 2.9817467],\n",
       "       [2.330052 , 2.9968555]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 张量计算图\n",
    "x = torch.rand(2,2,requires_grad=True)\n",
    "print(x,x.grad_fn)# grad_fn保存梯度，由于使叶子节点 没有梯度\n",
    "m = x+2\n",
    "print(m.grad_fn)# 具有fn属性\n",
    "print(m.grad_fn(x))# 对x的导数\n",
    "print(m.is_leaf)# 是否叶子节点\n",
    "print(x.is_leaf)\n",
    "f=m.mean()\n",
    "f.backward()#求导\n",
    "print(f,x.grad)#grad保存梯度值\n",
    "m.detach().numpy()#从图中分离转numpy"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
