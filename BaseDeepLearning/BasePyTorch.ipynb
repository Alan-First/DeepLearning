{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.9.0a0+gitd69c22d'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 用于检验pytorch安装是否成功的常规操作\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8360, 0.9117, 0.7324],\n",
       "        [0.2278, 0.6764, 0.2234],\n",
       "        [0.6801, 0.9425, 0.5561],\n",
       "        [0.0258, 0.2963, 0.0823],\n",
       "        [0.7544, 0.7261, 0.4810]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建Tensor的常用方式，常用于生成随机tensor作为输入测试搭好的神经网络模型内部是否维度匹配\n",
    "torch.rand(5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 常用创建tensor的方式\n",
    "x=torch.zeros(5,3,dtype=torch.long)# 创建全零张量，设置类型，还有其他可选torch.float64\n",
    "x=torch.Tensor([5,3])# 把列表[5,3]转化为张量\n",
    "y=x.new_ones(2,4)# 以x为模板创建一个2行4列的全1张量，这个张量与x有相同的torch.dtype和torch.device\n",
    "z=torch.randn_like(x,dtype=torch.float64)# 创建与x相同尺寸随机数\n",
    "x.size()# 获取tensor尺寸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.1860140562057495\n"
     ]
    }
   ],
   "source": [
    "x=torch.randn(1)\n",
    "print(x.item())# Tensor元素只有一个的时候用于获取Tensor的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.randn(2,2)\n",
    "y=torch.randn(2,2)\n",
    "z = torch.add(x,y)# 加法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor可以理解为一个变量，也就是说当创建一个Tensor(1)的时候，最后理解成创建了一个自变量x，x的取值为1，这样方便理解对该张量的求导，比如最后它与张量Tensor(2)相乘，就可以理解成另一个自变量y它相乘，得到函数z=x*y，这时候对Tensor(1)求导即为z对x求导，导数是y，而y=2，所以该导数为2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.4969,  0.3869,  0.1799],\n",
      "        [-0.2547,  0.6612,  0.5817],\n",
      "        [-0.1369,  1.0073, -0.4423],\n",
      "        [ 0.3169,  1.4155,  0.8150],\n",
      "        [ 0.0336,  0.6081, -1.0811]])\n",
      "tensor([-1.4969,  0.3869,  0.1799, -0.2547,  0.6612,  0.5817, -0.1369,  1.0073,\n",
      "        -0.4423,  0.3169,  1.4155,  0.8150,  0.0336,  0.6081, -1.0811])\n",
      "tensor([[-1.4969,  0.3869,  0.1799, -0.2547,  0.6612],\n",
      "        [ 0.5817, -0.1369,  1.0073, -0.4423,  0.3169],\n",
      "        [ 1.4155,  0.8150,  0.0336,  0.6081, -1.0811]])\n"
     ]
    }
   ],
   "source": [
    "x=torch.randn(5,3)\n",
    "z=x.view(15)# view函数得到的变量和x是共享内存的，也就是它们其实指向同一个Tensor，只不过观察角度不同\n",
    "z1=x.view(-1,5)\n",
    "print(x)\n",
    "print(z)# z是把x按行观察变成一个行向量\n",
    "print(z1)# view第一个参数-1说明按张量尺寸确定，第二个参数5说明列必须是5列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3]])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4]])\n",
      "tensor([[2, 3, 4],\n",
      "        [3, 4, 5],\n",
      "        [4, 5, 6],\n",
      "        [5, 6, 7]])\n"
     ]
    }
   ],
   "source": [
    "# 不同尺寸的Tensor相加时会自动触发广播机制，也就是尺寸向大尺寸对齐\n",
    "# 1）数组看齐维度最长的数组，维度不足部分在前加1补齐\n",
    "# 2）运算后的数组维度是所有数组维度的最大值\n",
    "# 3）输入数组的某个维度和输出数组的对应维度长度相同或长度为1，则数组可以计算，否则会出错\n",
    "# 4）输入数组某维度长度为1时，其他维度都用这一维度的值\n",
    "x=torch.arange(1,4).view(1,3)# 生成[1,4)的数\n",
    "y=torch.arange(1,5).view(4,1)\n",
    "print(x)\n",
    "print(y)\n",
    "print(x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "y= [[1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "x= tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "y= [[2. 2. 2.]\n",
      " [2. 2. 2.]\n",
      " [2. 2. 2.]\n",
      " [2. 2. 2.]\n",
      " [2. 2. 2.]]\n",
      "x= tensor([[3., 3., 3.],\n",
      "        [3., 3., 3.],\n",
      "        [3., 3., 3.],\n",
      "        [3., 3., 3.],\n",
      "        [3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "x=torch.ones(5,3)\n",
    "y=x.numpy()# 采用numpy方法将x转为numpy，二者内存共享\n",
    "print('x=',x)\n",
    "print('y=',y)\n",
    "x+=1\n",
    "print('x=',x)\n",
    "print('y=',y)# 可以发现我们只对x加1，但是y也随之改变，这是因为二者是共享内存的\n",
    "z=torch.from_numpy(y)# 该方法可以将numpy转Tensor，同样的，转出来的结果也共享内存\n",
    "z+=1\n",
    "print('x=',x)# 可以发现只对z加1，x的值却变了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.6977,  0.1552,  0.3303,  1.2030,  1.5942],\n",
      "        [ 0.6315,  1.3783,  2.1998,  0.5334,  1.2236],\n",
      "        [ 1.0338,  1.3130,  0.3557, -0.7227,  1.2405]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# pytorch在CPU和GPU的应用\n",
    "x=torch.randn(3,5)\n",
    "if torch.cuda.is_available():# 用于判断cuda是否可见和可用的常用判断\n",
    "    device=torch.device('cuda')# cuda设备设置\n",
    "    y=torch.ones_like(x,device=device)# 直接在device的GPU上创建Tensor\n",
    "    x=x.to(device)# 把x移动到cuda\n",
    "    z=x+y\n",
    "    print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "<AddBackward0 object at 0x7f1406fc1240>\n"
     ]
    }
   ],
   "source": [
    "# 自动求解梯度\n",
    "x=torch.randn(5,3,requires_grad=True)# 创建一个需要追踪梯度的张量\n",
    "# 这个用于查看创建该Tensor的Function（CalculateGraph.py有介绍这个函数），说明该变量是从哪个运算得到的\n",
    "# 由于我们直接创建，所以它是None\n",
    "print(x.grad_fn)\n",
    "y=x+1\n",
    "print(y.grad_fn)# y是自变量x加上常数得到的，通过加法创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad of x= False\n",
      "grad of z= True\n"
     ]
    }
   ],
   "source": [
    "# 修改张量求解梯度与否\n",
    "x=torch.randn(5,3)\n",
    "y=(x*3)/(x-1)# 默认创建时没有跟踪梯度\n",
    "print('grad of x=',x.requires_grad)\n",
    "x.requires_grad_(True)# 跟踪梯度\n",
    "z=(x*x).sum()# y可用求导\n",
    "print('grad of z=',z.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= tensor([[-0.4376,  1.3637,  0.3737],\n",
      "        [-1.2171,  0.4182,  0.8781],\n",
      "        [-0.1474,  0.8405, -1.4764],\n",
      "        [-0.1416, -1.6874,  0.6844],\n",
      "        [-0.0931, -1.3720, -0.3416]], requires_grad=True)\n",
      "x grad= tensor([[-0.8751,  2.7275,  0.7475],\n",
      "        [-2.4343,  0.8363,  1.7562],\n",
      "        [-0.2948,  1.6810, -2.9527],\n",
      "        [-0.2833, -3.3747,  1.3688],\n",
      "        [-0.1863, -2.7440, -0.6833]])\n"
     ]
    }
   ],
   "source": [
    "# 对上面的z求梯度\n",
    "z.backward()\n",
    "# x*x的导数是2*x，此时x已经有了梯度\n",
    "print('x=',x)\n",
    "print('x grad=',x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x**2 require grad= True\n",
      "x**2 require grad= False\n",
      "x require grad= True\n"
     ]
    }
   ],
   "source": [
    "x=torch.rand(5,3,requires_grad=True)\n",
    "print('x**2 require grad=',(x**2).requires_grad)\n",
    "with torch.no_grad():\n",
    "    print('x**2 require grad=',(x**2).requires_grad)# 用代码段以停止跟踪x接下来的运算的梯度\n",
    "    print('x require grad=',x.requires_grad)# 注意只是不跟踪x的运算的梯度，x的梯度还在\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文献7中的pytorch编程基础"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [0, 8, 0],\n",
       "       [0, 0, 4]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用基本的numpy\n",
    "# 创建对角矩阵\n",
    "v = np.array([1,8,4])\n",
    "np.diag(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.eye(3)# 创建单元矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(range(4)).reshape(2,2)\n",
    "b = np.array(range(4,8)).reshape(2,2)\n",
    "print(a*b)# 哈达玛积\n",
    "print(a@b)# 点积\n",
    "print(np.dot(a,b))# 点积\n",
    "ma = np.asmatrix(a)\n",
    "mb = np.asmatrix(b)\n",
    "print(ma*mb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
