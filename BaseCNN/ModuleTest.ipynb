{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- encoding: utf-8 -*-\n",
    "'''\n",
    "@File         :ModuleTest.ipynb\n",
    "@Description  :\n",
    "@Time         :2022/04/26 06:11:00\n",
    "@Author       :Hedwig\n",
    "@Version      :1.0\n",
    "'''\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from collections import OrderedDict\n",
    "import torch.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建简单的模型\n",
    "class LogicNet(nn.Module):\n",
    "    def __init__(self,inputdim,hiddendim,outputdim):\n",
    "        super(LogicNet,self).__init__()\n",
    "        self.Linear1 = nn.Linear(inputdim,hiddendim)# 创建全连接层\n",
    "        self.add_module(\"Linear2\",nn.Linear(hiddendim,outputdim))# 与上一行的创建方式无本质区别\n",
    "        # 还可以考虑更高级的ModuleList方法\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.Linear1(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.Linear2(x)\n",
    "        return x\n",
    "    \n",
    "    def predict(self,x):\n",
    "        pred = torch.softmax(self.forward(x),dim=1)\n",
    "        return torch.argmax(pred,dim=1)\n",
    "\n",
    "    def getloss(self,x,y):\n",
    "        y_pred = self.forward(x)\n",
    "        loss = self.criterion(y_pred,y)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=2, out_features=3, bias=True)\n",
      "Linear(in_features=3, out_features=2, bias=True)\n",
      "CrossEntropyLoss()\n",
      "Linear1 is: Linear(in_features=2, out_features=3, bias=True)\n",
      "Linear2 is: Linear(in_features=3, out_features=2, bias=True)\n",
      "criterion is: CrossEntropyLoss()\n",
      "LogicNet(\n",
      "  (Linear1): Linear(in_features=2, out_features=3, bias=True)\n",
      "  (Linear2): Linear(in_features=3, out_features=2, bias=True)\n",
      "  (criterion): CrossEntropyLoss()\n",
      ")\n",
      "Linear(in_features=2, out_features=3, bias=True)\n",
      "Linear(in_features=3, out_features=2, bias=True)\n",
      "CrossEntropyLoss()\n"
     ]
    }
   ],
   "source": [
    "# Module的children方法可以获取Module类实例的各层信息\n",
    "model = LogicNet(inputdim=2,hiddendim=3,outputdim=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.01)\n",
    "# 获取层信息\n",
    "for sub_module in model.children():\n",
    "    print(sub_module)\n",
    "# 获取层和名字信息\n",
    "for name,module in model.named_children():\n",
    "    print(name,'is:',module)\n",
    "# 获取整个网络的结构信息\n",
    "for module in model.modules():\n",
    "    print(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.Size([3, 2])\n",
      "<class 'torch.Tensor'> torch.Size([3])\n",
      "<class 'torch.Tensor'> torch.Size([2, 3])\n",
      "<class 'torch.Tensor'> torch.Size([2])\n",
      "<class 'torch.Tensor'> torch.Size([3, 2]) Linear1.weight\n",
      "<class 'torch.Tensor'> torch.Size([3]) Linear1.bias\n",
      "<class 'torch.Tensor'> torch.Size([2, 3]) Linear2.weight\n",
      "<class 'torch.Tensor'> torch.Size([2]) Linear2.bias\n"
     ]
    }
   ],
   "source": [
    "# Parameter类是Variable子类，模型中加入不同层的时候就会按该层的定义在模型中添加相应的参数\n",
    "# 这些参数都是可学习参数，以下方式是通过定义网络层以外的方式向模型添加参数\n",
    "# 为模型添加参数采用register_parameter(name,param)的方式添加\n",
    "# 这个方法把参数写进了网络中，跟随网络训练而训练\n",
    "# 神经网络搭建的时候有时需要保存一个状态，这个状态不是模型的参数(如BN层的均值和方差)\n",
    "# 这时候可以用register_buffer(name,tensor)为模型添加状态参数\n",
    "# 这个函数返回的不是模型的权重参数，他只是一个临时变量\n",
    "for param in model.parameters():# 用这个查看模型的参数\n",
    "    print(type(param.data),param.size())\n",
    "for name,param in model.named_parameters():\n",
    "    print(type(param.data),param.size(),name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param : tensor([-0.0240])\n",
      "buffer : tensor([[ 1.3605, -1.6510, -0.4707],\n",
      "        [-0.3103,  0.8497,  1.0285]])\n",
      "Linear1.weight : tensor([[0.6534],\n",
      "        [0.5043]])\n",
      "Linear1.bias : tensor([-0.8386,  0.4592])\n"
     ]
    }
   ],
   "source": [
    "# 模型定义时，属性中不限制变量的类型，也可以定义self.a=3诸如此类，\n",
    "# 但是在模型移植的时候我们只会加载可训练的参数，这些可训练参数可以用state_dict()函数获取\n",
    "# 它识别可学习参数是这样识别的：\n",
    "# Module子类的属性如果被赋予了Module子类、Parameter类或者buffer参数作为值，那么它是可学习参数\n",
    "class ModulePar(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModulePar,self).__init__()\n",
    "        # 作为Module子类ModulePar的属性Linear1被赋予了Module子类Linear作为值，它是可学习参数\n",
    "        self.Linear1 = nn.Linear(1,2)\n",
    "        # 作为Module子类ModulePar的属性tensor被赋予了一个不是以上内容的值，它不是可学习参数\n",
    "        self.tensor = torch.rand([1])\n",
    "        # 作为Module子类ModulePar的属性param被赋予了Parameter，它是可学习参数\n",
    "        self.param = nn.Parameter(torch.randn([1]))\n",
    "        # buffer参数，是可学习参数\n",
    "        self.register_buffer(\"buffer\",torch.randn([2,3]))\n",
    "model = ModulePar()\n",
    "for par in model.state_dict():\n",
    "    print(par,\":\",model.state_dict()[par])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (relu1): ReLU()\n",
      "  (conv2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (relu2): ReLU()\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.0844, -0.1884, -0.0736,  0.0814,  0.0067, -0.0674,  0.1926,  0.0472,\n",
       "         0.0425,  0.0572,  0.1535,  0.1007, -0.1479,  0.1875,  0.1376,  0.0156,\n",
       "        -0.0091, -0.1659, -0.1673, -0.0697])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 搭建卷积神经网络也可以用Sequential，它在简单网络里书写更加简洁，但是不太容易搭复杂的网络\n",
    "model = nn.Sequential(OrderedDict([\n",
    "    ('conv1',nn.Conv2d(1,20,5)),\n",
    "    ('relu1',nn.ReLU()),\n",
    "    ('conv2',nn.Conv2d(20,64,5)),\n",
    "    ('relu2',nn.ReLU())\n",
    "]))# 这个书写的时候甚至不用写forward函数，因为默认forward就是按着搭建的顺序连接\n",
    "print(model)\n",
    "print(model.state_dict()['conv1.weight'])\n",
    "print(model.state_dict()['conv1.bias'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.4166,  0.2200],\n",
      "        [-1.9129,  0.8044]])\n",
      "tensor([[-0.8889,  0.2166],\n",
      "        [-0.9573,  0.6665]])\n",
      "tensor([[-0.8889,  0.2166],\n",
      "        [-0.9573,  0.6665]])\n"
     ]
    }
   ],
   "source": [
    "# 激活层\n",
    "# 激活层都有两种形式，一种是类形式，一种是函数形式\n",
    "data = torch.randn(2,2)-0.5\n",
    "# 类形式如下初始化\n",
    "tanh = nn.Tanh()\n",
    "output = tanh(data)\n",
    "print(data)\n",
    "print(output)\n",
    "# 函数形式直接调用\n",
    "print(torch.tanh(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2正则化\n",
    "# Adam优化器中有个weight_decay参数，当对损失函数使用L2正则化的时候，求导结果相对于不适用正则化，\n",
    "# 权值w变化量相差了一个值，这个值为：学习率*正则化系数*权值，所以如果想使用正则化，在adam优化器\n",
    "# 的weight_decay参数设置一个值就好，这个值就是正则化系数\n",
    "# 推导可以查看：https://zhuanlan.zhihu.com/p/388415560"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout的实现同样包括类实现和函数实现,有Dropout、Dropout2D、Dropout3D三种实现\n",
    "# torch.dropout\n",
    "# Dropout的类实现不用设置training参数，它会自动识别是否是training状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 1.1638],\n",
      "          [-0.6488],\n",
      "          [-0.3210],\n",
      "          [-0.6156]],\n",
      "\n",
      "         [[-0.9440],\n",
      "          [-0.4823],\n",
      "          [-1.7982],\n",
      "          [ 0.1563]],\n",
      "\n",
      "         [[ 0.2894],\n",
      "          [ 0.1336],\n",
      "          [-0.2402],\n",
      "          [ 0.0348]]],\n",
      "\n",
      "\n",
      "        [[[-0.1778],\n",
      "          [-0.8765],\n",
      "          [ 0.1455],\n",
      "          [ 0.4631]],\n",
      "\n",
      "         [[-1.0449],\n",
      "          [-0.1946],\n",
      "          [ 0.4151],\n",
      "          [-1.4212]],\n",
      "\n",
      "         [[-0.7934],\n",
      "          [ 0.4222],\n",
      "          [-0.7207],\n",
      "          [ 0.7674]]]])\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0.], requires_grad=True)\n",
      "1e-05\n",
      "tensor([[[[ 2.0119],\n",
      "          [-0.8545],\n",
      "          [-0.3363],\n",
      "          [-0.8020]],\n",
      "\n",
      "         [[-0.3874],\n",
      "          [ 0.2519],\n",
      "          [-1.5699],\n",
      "          [ 1.1359]],\n",
      "\n",
      "         [[ 0.5944],\n",
      "          [ 0.2885],\n",
      "          [-0.4454],\n",
      "          [ 0.0947]]],\n",
      "\n",
      "\n",
      "        [[[-0.1098],\n",
      "          [-1.2146],\n",
      "          [ 0.4015],\n",
      "          [ 0.9039]],\n",
      "\n",
      "         [[-0.5270],\n",
      "          [ 0.6502],\n",
      "          [ 1.4942],\n",
      "          [-1.0480]],\n",
      "\n",
      "         [[-1.5316],\n",
      "          [ 0.8552],\n",
      "          [-1.3888],\n",
      "          [ 1.5330]]]], grad_fn=<NativeBatchNormBackward0>) torch.Size([2, 3, 4, 1])\n"
     ]
    }
   ],
   "source": [
    "# BN层也有类和函数两种实现，BatchNorm1d、BatchNorm2d、BatchNorm3d\n",
    "data = torch.randn(2,3,4,1)\n",
    "print(data)\n",
    "# 类实现如下，它不用设置training参数\n",
    "obn = nn.BatchNorm2d(3,affine=True)# 参数为图片通道数\n",
    "print(obn.weight)\n",
    "print(obn.bias)\n",
    "print(obn.eps)\n",
    "output = obn(data)\n",
    "print(output,output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型保存与加载\n",
    "torch.save(model.state_dict(),'./model.pth')# 保存模型\n",
    "# 模型载入，后面的map_location使得模型同时载入0号和1号卡，不常用\n",
    "model.load_state_dict(torch.load('./model.pth',map_location={'cuda:1':'cuda:0'}))\n",
    "\n",
    "# 多卡GPU中单卡训练，保存时最好用cpu方式存储，如果用上述方式存储，存下来的模型还会记录gpu卡号信息\n",
    "# 后面加载的时候需要用到\n",
    "model.cpu().state_dict()#单卡\n",
    "model.module.cpu().state_dict()#多卡\n",
    "\n",
    "# 如果不想存成cpu形式，那就加载的时候指派到指定的gpu\n",
    "# GPU1加载到GPU0\n",
    "torch.load('model.pth',map_location={'cuda:1':'cuda:0'})\n",
    "# GPU加载到cpu\n",
    "torch.load('model.pth',map_location=lambda storage,loc:storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 损失函数\n",
    "# 以下是L1损失的类实现的使用，MSELoss、CrossEntropyLoss也是类似\n",
    "# BCELoss是二分类，BCEWithLogitsLoss也是二分类损失，sigmoid+BCELoss，最后一层不使用Sigmoid时用这个函数\n",
    "# CrossEntropyLoss多分类损失，包括了求softmax的操作\n",
    "criterion = torch.nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['params', 'lr', 'betas', 'eps', 'weight_decay', 'amsgrad']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 优化器\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.01)\n",
    "# 查看优化器参数\n",
    "list(optimizer.param_groups[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学习率退化\n",
    "# 定长阶跃退化\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=50,gamma=0.99)# 每50步*0.99\n",
    "# 不定长阶跃,如下表示在200 700 800时*0.9\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,milestones=[200,700,800],gamma=0.9)\n",
    "# ReduceLROnPlateau退化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型： Model()\n",
      "输入： (tensor([0.4162], requires_grad=True),)\n",
      "输出： tensor(1.4162, grad_fn=<UnbindBackward0>)\n",
      "模型结果： tensor([1.4162], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 添加钩子函数\n",
    "# 钩子函数定义方式：register_forward_hook(hook)\n",
    "# 每次调用forward方法，这个hook都会被调用一次\n",
    "# hook函数定义为hook(module,input,output),\n",
    "# hook函数不能修改input跟output，它返回一个句柄handle，调用handle的remove方法可以将hook移除\n",
    "def for_hook(module,input,output):\n",
    "    print(\"模型：\",module)\n",
    "    for val in input:\n",
    "        print('输入：',input)\n",
    "    for out_val in output:\n",
    "        print('输出：',out_val)\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model,self).__init__()\n",
    "    def forward(self,x):\n",
    "        return x+1\n",
    "model = Model()\n",
    "x = torch.randn(1,requires_grad=True)\n",
    "handle = model.register_forward_hook(for_hook)\n",
    "print('模型结果：',model(x))\n",
    "# register_backward_hook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer.zero_grad方法一般每次迭代都要执行一次，硬件资源紧张的时候可以多次计算backward梯度以后\n",
    "# 执行一次优化器step然后zero_grad\n",
    "# 多任务场景下，分别各自的loss都backward完再相加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[-0.6788]]],\n",
      "\n",
      "\n",
      "        [[[ 0.6917]]]], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]]], requires_grad=True)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 卷积层的实现也包括函数实现和类实现\n",
    "#torch.nn.Conv2d()\n",
    "#torch.nn.functional.conv2d(inchannel,outchannel,kernel_size)\n",
    "# 一般卷积核参数都是随机初始化，如果想人为赋值，需要用类实现，如下\n",
    "condv = torch.nn.Conv2d(1,2,kernel_size=1,padding=1,bias=False)\n",
    "print(condv.weight)#(1,1,1,1)NCHW\n",
    "condv.weight = torch.nn.Parameter(torch.ones([2,1,1,1]))# 如果尺寸不一致则会覆盖 不会报错\n",
    "condv.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 池化层也有类实现和函数实现\n",
    "# 函数实现torch.nn.functional.max_pool1d、2d、3d或者avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多卡训练\n",
    "device_count = torch.cuda.device_count()\n",
    "print('cuda.device_count',device_count)\n",
    "device_ids = list(range(device_count))\n",
    "model = nn.DataParallel(model,device_ids=device_ids)\n",
    "criterion = nn.DataParallel(criterion,device_ids=device_ids)\n",
    "# 如果loss计算很消耗资源，建议将其放在forward方法中\n",
    "# 优化器不需要并行，因为参数会同步覆盖\n",
    "# 多卡保存模型代码\n",
    "torch.save(model.module.state_dict(),'model.pth')\n",
    "# 如果代码结束显存还占用，执行nvidia-smi -pm 1\n",
    "# 实时查看内存\n",
    "# pip install gpustat\n",
    "# watch --color -n1 gpustat -cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0014, 0.5008, 0.3068, 0.3104],\n",
      "        [0.9461, 0.3268, 0.9035, 0.9273]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [2]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 实现LSTM和GRU\n",
    "# torch.nn.LSTM\n",
    "# 输出结果形状为(序列长度，批次个数，方向*隐藏节点个数)\n",
    "# 隐藏层状态h、单元状态C形状为(方向*层数，批次个数，隐藏层节点个数)\n",
    "# torch.nn.GRU\n",
    "# 分布式采样接口\n",
    "data = torch.rand(2,4)#传入形状为(batch size,num class)的样本\n",
    "print(data)\n",
    "torch.multinomial(data,1)# 按numclass的分布取指定个数的样本"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
