{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import collections\n",
    "import os\n",
    "import urllib\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init embed Parameter containing:\n",
      "tensor([[ 1.7509, -0.7933, -0.9278,  0.0388, -0.4009],\n",
      "        [ 1.3134,  3.2451, -0.0717, -1.4094, -0.0504],\n",
      "        [ 0.1722, -0.6872,  0.6585, -0.2432,  0.5528],\n",
      "        [-0.9784,  0.7325,  1.1104,  0.2580,  0.4777]], requires_grad=True)\n",
      "word vector of word1 is tensor([[ 1.7509, -0.7933, -0.9278,  0.0388, -0.4009],\n",
      "        [ 1.3134,  3.2451, -0.0717, -1.4094, -0.0504],\n",
      "        [ 0.1722, -0.6872,  0.6585, -0.2432,  0.5528]],\n",
      "       grad_fn=<EmbeddingBackward>)\n",
      "new embed Parameter containing:\n",
      "tensor([[ 0.0918, -0.0733,  0.1528, -0.2699,  0.4226],\n",
      "        [ 0.4482, -0.0054, -0.1250,  0.4544, -0.2707],\n",
      "        [ 0.1023, -0.0611,  0.3351, -0.2793,  0.2026],\n",
      "        [-0.2983, -0.2673, -0.3272,  0.1086, -0.4682]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#torch.nn.Embedding(n,m)是用于词嵌入的,n是单词数，m是词向量维度数\n",
    "# 创建一个long类型的张量，这是一个索引张量，比如词袋中元素只有四个数\n",
    "# “你好吗我”在词袋中one-hot编码以后分别是0，1，2，3\n",
    "# 那么word1这个列表代表的就是“你好吗”这个含义\n",
    "word1 = torch.LongTensor([0,1,2])\n",
    "word2 = torch.LongTensor([3,1,2])\n",
    "# embedding则是创建一个参数可学习的随机张量矩阵，词袋有4个数，维度就是4，每个词向量维度是5\n",
    "# word1和word2分别用索引的方式获取每个字的词向量\n",
    "embedded = nn.Embedding(4,5)\n",
    "print('init embed',embedded.weight)\n",
    "print('word vector of word1 is',embedded(word1))#size为（3，5），表示3个字\n",
    "embedded.weight.data.uniform_(-0.5,0.5)\n",
    "print('new embed',embedded.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#一个batch的矩阵相乘\n",
    "batch_size = 3\n",
    "embed_size = 5\n",
    "context_num = 4\n",
    "inputs = torch.randn(batch_size,embed_size,1)\n",
    "mats = torch.randn(batch_size,context_num,embed_size)\n",
    "result = torch.bmm(mats,inputs)#batch_matrix_multiply\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 8])\n"
     ]
    }
   ],
   "source": [
    "mats = torch.randn(5,2,4)\n",
    "trans = mats.view(5,1,-1)\n",
    "print(trans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.154236   0.09939146 0.33624985 0.14808191 0.26204078]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "table = np.random.random(5)\n",
    "table /=np.sum(table)\n",
    "print(table)\n",
    "table_list = []\n",
    "count = np.round(100*table)\n",
    "for word_idx,c in enumerate(count):\n",
    "    table_list+=[word_idx]*int(c)\n",
    "print(table_list)\n",
    "print(len(table_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words= ['i', 'like', 'dog', 'i', 'like', 'cat', 'i', 'like', 'animal', 'dog', 'cat', 'animal', 'apple', 'cat', 'dog', 'like', 'dog', 'fish', 'milk', 'like', 'dog', 'cat', 'eyes', 'like', 'i', 'like', 'apple', 'apple', 'i', 'hate', 'apple', 'i', 'movie', 'book', 'music', 'like', 'cat', 'dog', 'hate', 'cat', 'dog', 'like']\n",
      "vocab2int= {'fish': 0, 'movie': 1, 'book': 2, 'music': 3, 'dog': 4, 'eyes': 5, 'like': 6, 'hate': 7, 'i': 8, 'apple': 9, 'milk': 10, 'cat': 11, 'animal': 12}\n",
      "int_words= [8, 6, 4, 8, 6, 11, 8, 6, 12, 4, 11, 12, 9, 11, 4, 6, 4, 0, 10, 6, 4, 11, 5, 6, 8, 6, 9, 9, 8, 7, 9, 8, 1, 2, 3, 6, 11, 4, 7, 11, 4, 6]\n",
      "word_freqs= {8: 0.14285714285714285, 6: 0.21428571428571427, 4: 0.16666666666666666, 11: 0.14285714285714285, 12: 0.047619047619047616, 9: 0.09523809523809523, 0: 0.023809523809523808, 10: 0.023809523809523808, 5: 0.023809523809523808, 7: 0.047619047619047616, 1: 0.023809523809523808, 2: 0.023809523809523808, 3: 0.023809523809523808}\n",
      "train_word= [8, 6, 4, 8, 6, 11, 8, 6, 12, 4, 11, 12, 9, 11, 4, 6, 4, 0, 10, 6, 4, 11, 5, 6, 8, 6, 9, 9, 8, 7, 9, 8, 1, 2, 3, 6, 11, 4, 7, 11, 4, 6]\n"
     ]
    }
   ],
   "source": [
    "# 对于NLP，文字初始化包括的操作有以下这些\n",
    "# 样本用例来自于https://github.com/wentsun12/NLP_Learning/blob/master/skip_gram/skip-gram.py\n",
    "text = \"I like dog i like cat i like animal dog cat animal apple cat dog like dog fish milk like dog \\\n",
    "cat eyes like i like apple apple i hate apple i movie book music like cat dog hate cat dog like\"\n",
    "FREQ = 0\n",
    "# 去除低频词\n",
    "def preprocess(text, FREQ):\n",
    "    text = text.lower()\n",
    "    words = text.split()\n",
    "    #去除低频词\n",
    "    word_counts = collections.Counter(words)\n",
    "    trimmed_words = [word for word in words if word_counts[word] > FREQ]\n",
    "    return trimmed_words\n",
    "words = preprocess(text, FREQ)\n",
    "print('words=',words)\n",
    "# 构建数字版句子\n",
    "vocab = set(words)\n",
    "vocab2int = {w: c for c, w in enumerate(vocab)}#词典\n",
    "int2vocab = {c: w for c, w in enumerate(vocab)}\n",
    "print('vocab2int=',vocab2int)\n",
    "#将文本转化为数值\n",
    "int_words = [vocab2int[w] for w in words]\n",
    "print('int_words=',int_words)\n",
    "# 统计特定句子的各个单词频率\n",
    "int_word_counts = collections.Counter(int_words)\n",
    "total_count = len(int_words)\n",
    "word_freqs = {w: c/total_count for w, c in int_word_counts.items()}\n",
    "print('word_freqs=',word_freqs)\n",
    "# 去除高频词\n",
    "DELETE_WORDS = False\n",
    "if DELETE_WORDS:\n",
    "    t = 1e-5\n",
    "    prob_drop = {w: 1-np.sqrt(t/word_freqs[w]) for w in int_word_counts}\n",
    "    train_words = [w for w in int_words if np.random.random()<(1-prob_drop[w])]\n",
    "else:\n",
    "    train_words = int_words  # [2,4,7,...]\n",
    "print('train_word=',train_words)\n",
    "# 改变词语频率，方便后面负采样\n",
    "word_freqs = np.array(list(word_freqs.values()))\n",
    "unigram_dist = word_freqs / word_freqs.sum()\n",
    "noise_dist = torch.from_numpy(unigram_dist ** (0.75) / np.sum(unigram_dist ** (0.75)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实际下载数据库操作大致如下，代码来自https://github.com/zhangxiann/Skip-gram\n",
    "url='http://mattmahoney.net/dc/'\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    if not os.path.exists(filename):\n",
    "        print('file not found')\n",
    "        filename, _ = urllib.request.urlretrieve(url+filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified', filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise  Exception('Failed to verify '+filename+'. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "#filename=maybe_download('text8.zip', 90112)#31344016\n",
    "def read_data(filename):\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        # 读取出来的每个单词是 bytes\n",
    "        data=f.read(f.namelist()[0]).split()\n",
    "        # 把 bytes 转换为 str\n",
    "        #data= [str(x, encoding = \"utf8\") for x in data]\n",
    "        data = list(map(lambda x: str(x, encoding = \"utf8\"), data))\n",
    "    return data\n",
    "\n",
    "words=read_data('text8.zip')# 理论上words里面全是句子\n",
    "\n",
    "print('Data size', len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCABULARY_SIZE=50000\n",
    "# 取出频数前 50000 的单词\n",
    "counts_dict = dict((collections.Counter(words).most_common(VOCABULARY_SIZE-1)))# 词语-频数对\n",
    "# 去掉频数小于 FREQ 的单词\n",
    "# trimmed_words = [word for word in words if counts_dict[word] > FREQ]\n",
    "# 计算 UNK 的频数 = 单词总数 - 前 50000 个单词的频数之和\n",
    "counts_dict['UNK']=len(words)-np.sum(list(counts_dict.values()))\n",
    "#建立词和索引的对应\n",
    "idx_to_word = [word for word in counts_dict.keys()]# 把词典所有词都记录下来\n",
    "word_to_idx = {word:i for i,word in enumerate(idx_to_word)}# 词语-idx对\n",
    "data = [word_to_idx.get(word,word_to_idx[\"UNK\"]) for word in words]# 一段文字变成一段idx句子，列表生成式\n",
    "# 计算单词频次\n",
    "total_count = len(data)\n",
    "word_freqs = {w: c/total_count for w, c in counts_dict.items()}\n",
    "# 以一定概率去除出现频次高的词汇\n",
    "if DELETE_WORDS:\n",
    "    t = 1e-5\n",
    "    prob_drop = {w: 1-np.sqrt(t/word_freqs[w]) for w in data}\n",
    "    data = [w for w in data if np.random.random()<(1-prob_drop[w])]\n",
    "else:\n",
    "    data = data\n",
    "#计算词频,按照原论文转换为3/4次方\n",
    "word_counts = np.array([count for count in counts_dict.values()],dtype=np.float32)\n",
    "word_freqs = word_counts/np.sum(word_counts)\n",
    "word_freqs = word_freqs ** (3./4.)\n",
    "word_freqs = word_freqs / np.sum(word_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-9f29ab8cb688>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m22\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m23\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m26\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m27\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m22\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m23\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m19\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not list"
     ]
    }
   ],
   "source": [
    "a=[22,23,24,25,26,27,22,23,19]\n",
    "b=[1,2,3]\n",
    "a[b]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
