# Word2Vec
## 词向量编码
1. 自然语言处理最先的一步是要将文字转换成机器可以识别和计算的数字，也就是词向量编码
2. 文本表示分为离散表示（独热编码、词袋模型等）和分布式表示，分布式表示包括词嵌入（word embedding）和共现矩阵（co-occurrence Matrix）等，词嵌入就包括word2vec、Glove等
3. 最开始采用的是独热编码(one-hot)，但是词汇表的扩大极易形成高维稀疏向量，带来维度灾难，同时无法衡量词语之间的相关性
4. 目前采用的word2vec包括跳字模型（Skip-Gram）和连续词袋模型（CBOW）
5. 为了解决词袋过大导致词向量编码模型输出维度过大的问题，主要采用层次softmax和负采样的方法
6. 